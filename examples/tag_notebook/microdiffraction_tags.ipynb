{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag images from a file system\n",
    "This notebook demonstrates how to use Splash-ML to scan a file system for files and then introduce them to splash-ml, how to save tag sets using the TagService in the tagging packages, and how to query on those tags.\n",
    "\n",
    "The notebook uses mongomock to mimic a mongo database instance in memory. This could easily be replaced with a MongoClient from pymongo.\n",
    "\n",
    "This notebook downloads and imports a pre-laballed data set that was generously provided by Carolin Sutter-Fella.\n",
    "\n",
    "<cite>\n",
    "    This data was acquired at beamline 12.3.2 at the Advanced Light\n",
    "Source, which is a DOE Office of Science User Facility under\n",
    "contract no. DE-AC02-05CH11231.\n",
    "</cite>\n",
    "\n",
    "The notebook downloads the dataset, which is a set of .tiff files and a csv that contains labels and the human tagger's confidence for each file. \n",
    "Lables include 'peaks', 'rings', 'rods' and 'arcs'. This notebook does the following:\n",
    "- Download sample files locally into a folder called 'data/labelled'\n",
    "- \"Anonymize\" the files, which copies them into a folder 'data/anonymous'\n",
    "- Setups a TagService instance and introduce those files from 'data/anonymous' and their labels from the labels.csv file\n",
    "- Demonstrates querying for those tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../..')\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from mongomock import MongoClient\n",
    "from tagging.model import FileDataset, TagSource, TaggingEvent, DatasetType, Tag, LABEL_NAME\n",
    "from tagging.tag_service import TagService\n",
    "from tagging.util.files import anonymize_copy\n",
    "\n",
    "# By default, this does not require a mongo server running, but can use the mongomock\n",
    "# library to create an in-memory simulation of mongo. This will be deleted when the \n",
    "# kernel is shutdown. \n",
    "in_memory_db = True\n",
    "if in_memory_db:\n",
    "    from mongomock import MongoClient\n",
    "else:\n",
    "    # for now, expects mongo running on localhost:27\n",
    "    from pymongo import MongoClient\n",
    "\n",
    "src_root_path = os.path.join(\"data\", \"labelled\")\n",
    "src_relative_path = \"labelled\"\n",
    "dest_root = os.path.join(\"data\", \"anonymous\", src_relative_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_zip():\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    link = \"https://portal.nersc.gov/cfs/als/splash_ml/labelled.zip\"\n",
    "    file_name = \"data/labelled.zip\"\n",
    "\n",
    "    if os.path.exists('data'):\n",
    "        print(\"Directory data/labelled exists...skipping\")\n",
    "        \n",
    "    else:\n",
    "        os.mkdir('data')\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            print(f\"downloading {link} to {file_name}\")\n",
    "            response = requests.get(link, stream=True)\n",
    "            total_length = int(response.headers.get('content-length'))\n",
    "            chunk_size = 4096  # 1 MB\n",
    "            num_bars = int(total_length / chunk_size)\n",
    "            print(total_length)\n",
    "            if total_length is None:\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                for chunk in tqdm(\n",
    "                    response.iter_content(chunk_size=chunk_size), total=num_bars, unit='KB', desc=\"labelled.zip\", leave=True, file=sys.stdout):\n",
    "                    file.write(chunk)\n",
    "\n",
    "    with ZipFile('data/labelled.zip', 'r') as zipObj:\n",
    "        # Extract all the contents of zip file in different directory\n",
    "        zipObj.extractall('data')\n",
    "        print('File is unzipped into  \"data/labelled\" folder') \n",
    "\n",
    "download_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = MongoClient()\n",
    "\n",
    "#use glob to find all the files to ingest\n",
    "paths = glob.glob('data/labelled/**/*.tif*', recursive=True)\n",
    "\n",
    "# tag_svc instance to be used throughout creating and querying tags\n",
    "tag_svc = TagService(db, db_name='tagging')\n",
    "\n",
    "# tagger represents the entity creates tags on the assets...in this case, it's us!\n",
    "tagger = tag_svc.create_tag_source(TagSource(type=\"human\", name=\"build_tag notebook\"))\n",
    "\n",
    "# event is recorded with each tag on each asset so we know when and what created the tags, in this case, it's us and now!\n",
    "tagging_event = tag_svc.create_tagging_event(TaggingEvent(tagger_id=tagger.uid, run_time=datetime.now()))\n",
    "\n",
    "# read the csv for tags\n",
    "labels = pd.read_csv(\"data/labelled/labels.csv\", header=[0])\n",
    "\n",
    "num_files = 0\n",
    "for src_root_path in paths:\n",
    "    anonymous_file = anonymize_copy(src_root_path, src_relative_path, dest_root)\n",
    "    # We'll take advantage of the fact that the anonymous file name is unique (hash of the file)\n",
    "    anonymous_file_name = os.path.splitext(os.path.split(anonymous_file)[1])[0]\n",
    "    # get label row from csv using file name\n",
    "    file_name = os.path.splitext(os.path.split(src_root_path)[1])[0]\n",
    "    \n",
    "    3 # An Dataset is a reference in the tagging database that stores information about something being tagged as well\n",
    "    # as its tags. In this case, we have a file, so we're creating a FileDataset. Databroker assets will be available soon!\n",
    "    # We pass the anonymous file's name, which is a hash of the file itself, as the uid. This is optional, a uid will be created\n",
    "    # if none is passed.\n",
    "    asset = FileDataset(uid=anonymous_file_name, uri=anonymous_file)\n",
    "    # Associate this file with the name listed in the csv\n",
    "    row = labels.loc[labels['image name'] == int(file_name)]\n",
    "    tags = []\n",
    "    tags.append(Tag(name=LABEL_NAME, value=\"peaks\", confidence=row['peaks'].values[0], event_id=tagging_event.uid))\n",
    "    tags.append(Tag(name=LABEL_NAME, value=\"rings\", confidence=row['rings'].values[0], event_id=tagging_event.uid))\n",
    "    tags.append(Tag(name=LABEL_NAME, value=\"rods\", confidence=row['rods'].values[0], event_id=tagging_event.uid))\n",
    "    tags.append(Tag(name=LABEL_NAME, value=\"arcs\", confidence=row['arcs'].values[0], event_id=tagging_event.uid))\n",
    "    asset.tags = tags\n",
    "    tag_svc.create_dataset(asset)\n",
    "    num_files += 1\n",
    "print(f\"Anonymized, imported and tagged {num_files} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the tagging database, we can do some queries on what we have. First, find random tagging events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assets_with_peaks = tag_svc.find_datasets(**{\"tags.value\": \"peaks\"})\n",
    "for asset in assets_with_peaks:\n",
    "    tag = [tag for tag in asset.tags if tag.value == \"peaks\"]\n",
    "    print(f\"uid: {asset.uid} at {asset.uri} has peak tag: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We query based on tags. (note that this signature will be enhanced to make confidence parameters a range and optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('env')",
   "language": "python",
   "name": "python38564bitenv8fa1de97d37e40a1a97c405ac34e33d2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
