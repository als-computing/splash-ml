{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag images from a file system\n",
    "This notebook demonstrates how to use Splash-ML to scan a file system for files and then introduce them to splash-ml, how to save tag sets using the TagService in the tagging packages, and how to query on those tags.\n",
    "\n",
    "The notebook uses mongomock to mimic a mongo database instance in memory. This could easily be replaced with a MongoClient from pymongo.\n",
    "\n",
    "This notebook downloads and imports a pre-laballed data set that was generously provided by Carolin Sutter-Fella.\n",
    "\n",
    "<cite>\n",
    "    This data was acquired at beamline 12.3.2 at the Advanced Light\n",
    "Source, which is a DOE Office of Science User Facility under\n",
    "contract no. DE-AC02-05CH11231.\n",
    "</cite>\n",
    "\n",
    "The notebook downloads the dataset, which is a set of .tiff files and a csv that contains labels and the human tagger's confidence for each file. \n",
    "Lables include 'peaks', 'rings', 'rods' and 'arcs'. This notebook does the following:\n",
    "- Download sample files locally into a folder called 'data/labelled'\n",
    "- \"Anonymize\" the files, which copies them into a folder 'data/anonymous'\n",
    "- Setups a TagService instance and introduce those files from 'data/anonymous' and their labels from the labels.csv file\n",
    "- Demonstrates querying for those tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../..')\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from mongomock import MongoClient\n",
    "from tagging.model import FileDataset, TagSource, TaggingEvent, DatasetType, Tag\n",
    "from tagging.tag_service import TagService\n",
    "from tagging.util.files import anonymize_copy\n",
    "\n",
    "# By default, this does not require a mongo server running, but can use the mongomock\n",
    "# library to create an in-memory simulation of mongo. This will be deleted when the \n",
    "# kernel is shutdown. \n",
    "in_memory_db = True\n",
    "if in_memory_db:\n",
    "    from mongomock import MongoClient\n",
    "else:\n",
    "    # for now, expects mongo running on localhost:27\n",
    "    from pymongo import MongoClient\n",
    "\n",
    "src_root_path = os.path.join(\"data\", \"labelled\")\n",
    "src_relative_path = \"labelled\"\n",
    "dest_root = os.path.join(\"data\", \"anonymous\", src_relative_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_zip():\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    link = \"https://portal.nersc.gov/cfs/als/splash_ml/labelled.zip\"\n",
    "    file_name = \"data/labelled.zip\"\n",
    "\n",
    "    if os.path.exists('data'):\n",
    "        print(\"Directory data/labelled exists...skipping\")\n",
    "        \n",
    "    else:\n",
    "        os.mkdir('data')\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            print(f\"downloading {link} to {file_name}\")\n",
    "            response = requests.get(link, stream=True)\n",
    "            total_length = int(response.headers.get('content-length'))\n",
    "            chunk_size = 4096  # 1 MB\n",
    "            num_bars = int(total_length / chunk_size)\n",
    "            print(total_length)\n",
    "            if total_length is None:\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                for chunk in tqdm(\n",
    "                    response.iter_content(chunk_size=chunk_size), total=num_bars, unit='KB', desc=\"labelled.zip\", leave=True, file=sys.stdout):\n",
    "                    file.write(chunk)\n",
    "\n",
    "    with ZipFile('data/labelled.zip', 'r') as zipObj:\n",
    "        # Extract all the contents of zip file in different directory\n",
    "        zipObj.extractall('data')\n",
    "        print('File is unzipped into  \"data/labelled\" folder') \n",
    "\n",
    "download_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = MongoClient()\n",
    "\n",
    "#use glob to find all the files to ingest\n",
    "paths = glob.glob('data/labelled/**/*.tif*', recursive=True)\n",
    "\n",
    "# tag_svc instance to be used throughout creating and querying tags\n",
    "tag_svc = TagService(db, db_name='tagging')\n",
    "\n",
    "# tagger represents the entity creates tags on the assets...in this case, it's us!\n",
    "tagger = tag_svc.create_tag_source(TagSource(type=\"human\", name=\"build_tag notebook\"))\n",
    "\n",
    "# event is recorded with each tag on each asset so we know when and what created the tags, in this case, it's us and now!\n",
    "tagging_event = tag_svc.create_tagging_event(TaggingEvent(tagger_id=tagger.uid, run_time=datetime.now()))\n",
    "\n",
    "# read the csv for tags\n",
    "labels = pd.read_csv(\"data/labelled/labels.csv\", header=[0])\n",
    "\n",
    "num_files = 0\n",
    "for src_root_path in paths:\n",
    "    anonymous_file = anonymize_copy(src_root_path, src_relative_path, dest_root)\n",
    "    # We'll take advantage of the fact that the anonymous file name is unique (hash of the file)\n",
    "    anonymous_file_name = os.path.splitext(os.path.split(anonymous_file)[1])[0]\n",
    "    # get label row from csv using file name\n",
    "    file_name = os.path.splitext(os.path.split(src_root_path)[1])[0]\n",
    "    \n",
    "    3 # An Dataset is a reference in the tagging database that stores information about something being tagged as well\n",
    "    # as its tags. In this case, we have a file, so we're creating a FileDataset. Databroker assets will be available soon!\n",
    "    # We pass the anonymous file's name, which is a hash of the file itself, as the uid. This is optional, a uid will be created\n",
    "    # if none is passed.\n",
    "    asset = FileDataset(uid=anonymous_file_name, uri=anonymous_file)\n",
    "    # Associate this file with the name listed in the csv\n",
    "    row = labels.loc[labels['image name'] == int(file_name)]\n",
    "    tags = []\n",
    "    tags.append(Tag(name=\"peaks\", confidence=row['peaks'].values[0], event_id=tagging_event.uid))\n",
    "    tags.append(Tag(name=\"rings\", confidence=row['rings'].values[0], event_id=tagging_event.uid))\n",
    "    tags.append(Tag(name=\"rods\", confidence=row['rods'].values[0], event_id=tagging_event.uid))\n",
    "    tags.append(Tag(name=\"arcs\", confidence=row['arcs'].values[0], event_id=tagging_event.uid))\n",
    "    asset.tags = tags\n",
    "    tag_svc.create_dataset(asset)\n",
    "    num_files += 1\n",
    "print(f\"Anonymized, imported and tagged {num_files} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the tagging database, we can do some queries on what we have. First, find random tagging events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assets_with_peaks = tag_svc.find_datasets(tags=[\"peaks\"])\n",
    "for asset in assets_with_peaks:\n",
    "    tag = [tag for tag in asset.tags if tag.name == \"peaks\"]\n",
    "    print(f\"uid: {asset.uid} at {asset.uri} has peak tag: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We query based on tags. (note that this signature will be enhanced to make confidence parameters a range and optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('env')",
   "language": "python",
   "name": "python38564bitenv8fa1de97d37e40a1a97c405ac34e33d2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
